---
title: "IGNITE Zagreb workshop: Introduction to Statistics"
output: html_notebook
author: Maja Kuzman
date: "`r Sys.Date()`" 
---



## Introduction 

```{r,results=FALSE,echo=FALSE}
set.seed(1) #so that we get same results
```

This chapter introduces the statistical concepts necessary to understand p-values and confidence intervals. These terms are ubiquitous in the life science literature. Let's use [this paper](http://diabetes.diabetesjournals.org/content/53/suppl_3/S215.full) as an example. 

Imagine that we actually have the weight of the entire population of mice and can upload them to R. In Statistics, we refer to this as *the population*. Note that in practice we do not have access to the population. We have a special dataset that we are using here to illustrate concepts. 

```{r echo=FALSE, results="hide"}
library(data.table)
population <- fread("https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/mice_pheno.csv")
population
```

#### Our first look at data


```{r}
SampleThePopulation <- function(n, seed=1.25, onlyfemale=T, nullhyp = FALSE){
    if (seed!=1.25) set.seed(seed)
    pop <- copy(population)
    if(nullhyp==TRUE) {
        n=n*2
        pop <- pop[Diet=="chow"]
        pop[sample(1:nrow(pop),nrow(pop)/2),Diet:="chow2"]
        }
    if(onlyfemale==F){
        mice <- pop[,.SD[sample(1:.N,round(n/4))],.(Sex,Diet)]
    }
    
    if(onlyfemale==T){
        mice <- pop[Sex=="F",.SD[sample(1:.N,round(n/2))],.(Diet)]
    }
    mice
}

```

We are interested in determining if following a given diet makes mice heavier after several weeks. This data was produced by ordering 24 mice from The Jackson Lab and randomly assigning either chow or high fat (hf) diet. The function SampleThePopulation simulates the experiment for us.  It takes the number of mice you wish to order from the Jackson Lab and randomly assigns half of them to control (chow) diet and half of them to  hihgh fat (hf) diet and returns the bodyweight of mice after several weeks. (By default it will only give you the female mice, since this might influence the results.) 

Note:
To make sure that we all work with the same sample, we use "seed". By changing the seed to any number 
you get a different sample. Same seed and the same size will always give the same subsample from the population.


After several weeks, the scientists weighed each mouse and obtained this data:

```{r}

oneExperiment <- SampleThePopulation(24,seed = 1)
oneExperiment

``` 

In RStudio, you can view the entire dataset with:

```{r,eval=FALSE}
View(oneExperiment)
```


So are the hf mice heavier? Mouse 4 at 18.35 grams is one of the
lightest mice, while Mouse 7 at 39.42 grams is one of the heaviest from the female mice. Both are on
the hf diet. Just from looking at the data, we see there is
*variability*. 

One way to compare two groups is to plot all the values in one group in a histogram and all the values from the other group as well in the same plot. In R, this is done easily like this:
```{r}
library(ggplot2)

ggplot(oneExperiment, aes(Bodyweight)) +
    geom_histogram()

```

If you want to see the result by groups you can add fill= and the name of the variable you wish to group by. So lets do it by Diet variable:

```{r}

ggplot(oneExperiment, aes(Bodyweight, fill=Diet)) +
    geom_histogram()

```

Another way to see this is to use a density plot. Do this by copy pasting the previous code and changing geom_histogram() to geom_density(). If you want to make the plot transparent add alpha=0.5 inside the geom_density brackets.


```{r}

ggplot(oneExperiment, aes(Bodyweight, fill=Diet)) +
    geom_density(alpha=0.5)

```

Can you based on this conclude if the weights are different for two groups?

Other common representation of such data is to use a boxplot. Note that now we need to provide a variable for x axis (Diet) and for the y axis (Bodyweight). Would you say that the groups have different weights based on this?


```{r}

ggplot(oneExperiment, aes(Diet, Bodyweight, fill=Diet)) +
    geom_boxplot()
    
```



It is not easy to decide if two variables have the same distribution. This is why we usually refer to the averages. So let's look at the average of each group: 

```{r,message=FALSE}

control <- oneExperiment[Diet=="chow", Bodyweight]
treatment <- oneExperiment[Diet=="hf", Bodyweight]
print( mean(treatment) )
print( mean(control) )


```

Are high fat diet mice heavier on average then the control group? How much? Compare those values!

```{r}
mean(treatment)/mean(control)
obsdiff <- mean(treatment)-mean(control)
obsdiff
```


So the hf diet mice are about 10% heavier. Are we done? Why do we need p-values and confidence intervals? 

The reason is that these averages are random variables. They can take many values. What does this mean? If we repeat the experiment, we obtain 24 new mice from The Jackson Laboratory and, after randomly assigning them to each diet, we get a different mean. 

Try this yourself! 

1. Order a new sample form Jackson lab of 24 female mice. Set seed to 18 so we get a different sample then previous time. To do this, replace the values ??? with proper values:

```{r}
Experiment2 <- SampleThePopulation(n= 24, seed = 18)

```

2. Divide the dataset into control2 and treatment2 variables which contain the weights for the mice in this experiment.
```{r, eval=FALSE}
#treatment2 <- ???
#control2 <- ???

```

3. Calculate and compare the means of the groups now. How different are the weights now? What does this mean? 

```{r}

```




Every time we repeat this experiment, we get a different value. We call this type of quantity a *random variable*. 

For convenience, I made a function which will give us the observed difference between the group means:

```{r}
getObsDiff <- function(n,seed=1.25, nullhyp=FALSE){
    oneExperiment <- SampleThePopulation(n = n, seed = seed, nullhyp = nullhyp)
    control <- oneExperiment[Diet=="chow", Bodyweight]
    if(nullhyp){
        treatment <- oneExperiment[Diet=="chow2", Bodyweight]
    }else {
        treatment <- oneExperiment[Diet=="hf", Bodyweight]
    }
    obsdiff <- mean(treatment) - mean(control)
    obsdiff
}

```

This is how the previous examples look like if we plot them:

Example with seed=1, 18

```{r}
oneExperiment <- SampleThePopulation(n = 24, seed = 1)
ggplot(oneExperiment, aes(Diet, Bodyweight, fill=Diet))+geom_boxplot()

oneExperiment <- SampleThePopulation(n = 24, seed = 18)
ggplot(oneExperiment, aes(Diet, Bodyweight, fill=Diet))+geom_boxplot()

```
and the difference in the means for this samples was:

```{r}
getObsDiff(24,1)
getObsDiff(24,18)

```

Example for random seeds:

```{r}
somerandomseed <- sample(1:100000,1)
oneExperiment <- SampleThePopulation(n = 24, seed = somerandomseed)
ggplot(oneExperiment, aes(Diet, Bodyweight, fill=Diet))+geom_boxplot()
getObsDiff(24,somerandomseed)

```



Notice two things here:

1. instead of looking at the mean for control and the mean for the treatment group, we can simply look at the difference between them. This is often done in statistics. If the difference between the means is close to 0, this indicates that the two groups are not so different. If the difference between the group means is large, this indicates that the groups can not be so similar. It is important to see that the difference between the groups depends on the sample we take. It changes as we take different samples. So it is a random variable too :).

Now, instead of checking if two groups are different, it is enough to check if the difference between the group means is different from 0. If it is larger then 0 for most of the samples, this means that most of the samples have different means in control and treatment, which indicates that the treatment does effect the bodyweight differently then the control!

2. When we get random samples if the other group is the control group, there would still be difference in means!! In other words, means differ between two control samples. 




## Random Variables

Let's explore random variables further. 

Set the seed to different values to see how the average difference between the groups changes. Use seeds 5, 10 and 15. (This numbers are random and you can use any number you wish but the result you get will be different from your collegues if you use different values then them.)

```{r}

getObsDiff(24,5)
getObsDiff(24,10)
getObsDiff(24,15)

```


Note how the average difference between the groups varies a lot! We can continue to do this repeatedly and start learning something about the distribution of this random variable!


So can we conclude that it is larger then zero in general (meaning the control and treatment averages are different in general)? 
Would this difference still appear if we took 2 random samples and didn't treat them? Would this difference be so large then?



## The Null Hypothesis

Now let's go back to the second point made earlier!
As scientists we need to be skeptics. How do we know that this `obsdiff`
is due to the diet? What happens if we give all 24 mice the same diet? Will
we see a difference this big? Statisticians refer to this scenario as
the *null hypothesis*. The name "null" is used to remind us that we
are acting as skeptics: we give credence to the possibility that there
is no difference.

Because we have access to the population, we can actually observe as
many values as we want of the difference of the averages when the diet
has no effect. We can do this by randomly sampling 24 control mice,
giving them the same diet, and then recording the difference in mean
between two randomly split groups of 12 and 12. For convenience, you can
simply do this by changing the parameter nullhyp to TRUE in functions SampleThePopulation and getObsDiff!

```{r}

somerandomseed <- sample(1:100000,1)
oneExperiment <- SampleThePopulation(n = 24, seed = somerandomseed, nullhyp = T)
ggplot(oneExperiment, aes(Diet, Bodyweight, fill=Diet))+geom_boxplot()
getObsDiff(24,somerandomseed, nullhyp = T)


```


So the question we are actually interested in is:
Is the difference between control and treatment sample larger then difference between different control samples?


Now let's do it 10,000 times and record the differences between the controls and control.

this takes a long time to run...

```{r}
controldifference <-  replicate(10000,getObsDiff(24, nullhyp = T))
```

The values in `controldifference` form what we call the *null distribution*. We will define this more formally below.

```{r}
plot(density(controldifference))
hist(controldifference, xlim=c(-4,10))
abline(v=getObsDiff(24,1), col=2)
abline(v=getObsDiff(24,18), col=3)

```



In the first example, with seed=1 we got the difference between bontrol and treatment to be 2.4875.
```{r}
obsdiff <- getObsDiff(24, seed=1)

```




So what percent of the 10,000 are bigger than `obsdiff`?

```{r}
mean(controldifference >= obsdiff)

mean(abs(controldifference) >= obsdiff)


```

Only a 0.61% of the 10,000 simulations. As skeptics what do
we conclude? When there is no diet effect, we see a difference as big
as the one we observed only 0.6% of the time. This is what is known as
a p-value, which we will define more formally later.


Now try to take 100 samples from the null distribution and measure the p value! 

```{r}
controldifference100 <- replicate(100,getObsDiff(24, nullhyp = T))

hist(controldifference100, xlim=c(-4,10))
abline(v=getObsDiff(24,1), col=2)
abline(v=getObsDiff(24,18), col=3)

mean(abs(controldifference100) >= obsdiff)

```




COOOL! This means we can calculate p values without any tests now!!!

(Well.. yes. If you can take 10000 samples from the population and measure them! ... But there IS something cool about this: )

### Central limit theorem.

Central limit theorem tells us that the averages of samples will follow a normal distribution with mean same as the mean of the sample, but standard deviation will be equal to st. dev. of the sample divided by the square root of the size of the sample!


Note : for samples with size smaller then around 30, the averages follow a t distribution with those means and variances.


...

okay.

...

Why is this cool?



```{r}
oneExperiment <- SampleThePopulation(24,1)
control<-oneExperiment[Diet=="chow", Bodyweight]
newmean = mean(control)
newsd = sd(control)/sqrt(24)
samplemeans1 = rnorm(100000,newmean, newsd)
samplemeans2 = rnorm(100000,newmean, newsd)
nulldistribution <- samplemeans1-samplemeans2
plot(density(nulldistribution))
abline(v=obsdiff, col=2)
mean(nulldistribution>obsdiff)

```


And now, since the normal distribution is very well described ( Actually, you only need mean and standard deviation and you can say what is the surface between any two numbers on x.) we can calculate the p value just by knowing the mean and standard deviation, by using a mathematical formula (or a function in R), and withouth the need to get random samples anymore! 


```{r}
1-pnorm(obsdiff, mean(nulldistribution), sd(nulldistribution))

```


You can see that calculation of p value is nothing more then taking random samples and calculating something from them.

-------------------------------------------------------------------------------------------------------------------------
#### Note on normal distribution

## Distributions

We have explained what we mean by *null* in the context of null hypothesis, but what exactly is a distribution?
The simplest way to think of a *distribution* is as a compact description of many numbers. 

## Probability Distribution

Summarizing lists of numbers is one powerful use of distribution. An even more important use is describing the possible outcomes of a
random variable. Unlike a fixed list of numbers, we don't actually observe all possible outcomes of random variables, so instead of describing proportions, we describe probabilities. 

For example, in the case above, if we know the distribution of the difference in mean of mouse weights when the null hypothesis is true, referred to as the _null distribution_, we can compute the probability of observing a value as large as we did,referred to as a_p-value_. In a previous section we ran what iscalled a _Monte Carlo_ simulation (we will provide more details onMonte Carlo simulation in a later section) and we obtained 10,000 outcomes of the random variable under the null hypothesis.  Let's repeat the loop above, but this time let's add a point to the figure every time we re-run the experiment. If you run this code, you can see the null distribution forming as the observed values stack on top of each other.  

```{r}
#run in console!
n <- 100

plot(0,xlim=c(-6,6),ylim=c(1,30), xlab="Observed differences (grams)", ylab="Frequency")
totals <- vector("numeric",11)
entirepopulation <- population$Bodyweight[population$Diet=="chow"]
for (i in 1:n) {
  control <- sample(entirepopulation,12)
  treatment <- sample(entirepopulation,12)
  nulldiff <- mean(treatment) - mean(control)
  j <- pmax(pmin(round(nulldiff)+6,11),1)
  totals[j] <- totals[j]+1
  text(j-6,totals[j],pch=15,round(nulldiff,1))
  if(i < 10) Sys.sleep(0.8) 
}

```

The figure above amounts to a histogram. From a histogram of the
`controldifference` vector we calculated earlier, we can see that values as large
as `obsdiff` are relatively rare: 

```{r null_and_obs,fig.cap="Null distribution with observed difference marked with vertical red line."}
hist(controldifference, freq=TRUE)
abline(v=obsdiff, col="red", lwd=2)
```

An important point to keep in mind here is that while we defined $\mbox{Pr}(a)$ by counting cases, we will learn that, in some circumstances, mathematics gives us formulas for $\mbox{Pr}(a)$ that save us the trouble of computing them as we did here. One example of this powerful approach uses the normal distribution approximation.


## Normal Distribution

The probability distribution we see above approximates one that is very common in nature: the bell curve, also known as the normal distribution or Gaussian distribution. When the histogram of a list of numbers approximates the normal distribution, we can use a convenient mathematical formula to approximate the proportion of values or outcomes in any given interval:

$$
\mbox{Pr}(a < x < b) = \int_a^b \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left( \frac{-(x-\mu)^2}{2 \sigma^2} \right)} \, dx
$$

While the formula may look intimidating, don't worry, you will never actually have to type it out, as it is stored in a more convenient
form (as `pnorm` in R which sets *a* to $-\infty$, and takes *b* as an argument).
 
Here $\mu$ and $\sigma$ are referred to as the mean and the standard deviation of the population (we explain these in more detail in another section). If this *normal approximation* holds for our list, then the population mean and variance of our list can be used in the formula above. An example of this would be when we noted above that only 1.5% of values on the null distribution were above `obsdiff`. We can compute the proportion of values below a value `x` with `pnorm(x,mu,sigma)` without knowing all the values. The normal approximation works very well here:  
```{r}
1 - pnorm(obsdiff,mean(controldifference),sd(controldifference)) 
```

Later, we will learn that there is a mathematical explanation for this. A very useful characteristic of this approximation is that one only needs to know $\mu$ and $\sigma$ to describe the entire distribution. From this, we can compute the proportion of values in any interval. 

-------------------------------------------------------------------------------------------------------------------------

####Exercise:
### Hypothesis testing: a single case

Let's say you know in advance that the mean IQ score in the population is 100 and the standard deviation is 15 (as in the Wechsler Adult Intelligence Scale). You have an idea that listening to classical music increases intelligence --- even listening to a single piece of music. You get your sister to listen to Beethoven's Ninth and then measure her IQ. Her IQ score after listening to Beethoven's Ninth is 140. Did the treatment work?

Note that this is not an example of a good experiment. Obviously it would be helpful to know her IQ before listening to the music and then see if it changed.

Hypothesis testing operates by answering the question:

> What is the probability that we could have observed a score that high under the null hypothesis that the treatment had no effect on IQ?


Said in a different way, what is the probability that we could have observed a score that high by randomly sampling from the population of individuals who have not undergone the treatment?

Again, this is a bad example of an experiment, obviously in the general population some individuals have listened to classical music before.

The probability equals the area under the normal curve to the right of
the IQ=140 point:

```{r}
xr <- 25:175
xrl <- length(xr)
iq <- 140
pline <- dnorm(x=xr, mean=100, sd=15)
plot(xr, pline, type="l", xlab="IQ", ylab="probability")
lines(c(min(xr), max(xr)), c(0,0))
lines(c(iq,iq), c(0,.03), col="red")
hb <- max(which(xr<=iq))
polygon(c(xr[hb:xrl], rev(xr[hb:xrl])), c(pline[hb:xrl], rep(0, xrl-hb+1)), col="red")

```


What are the chances we would have observed an IQ of 140 in an individual randomly sampled from a population who was *not* affected by the treatment?

````{r}


```

What do you conclude about the treatment?



If we didn't have a hypothesis *a priori* about the direction of the effect of the treatment (whether it should increase or decrease IQ) then we would need to divide our $\alpha$ level by two---because we would be doing a *two-tailed* test. The above test was a *one-tailed* test because we had a hypothesis in advance that the treatment should raise IQ.


## Hypothesis testing: a single group

Now let's say we decide to test 20 people instead of just one. We randomly sample them from the population, apply our treatment, observe that the mean IQ of our sample was 113.3, and we ask the same question:

```{r}
set.seed(1)
peoplesIQs <- sample(100:130,20, replace = T)
mean(peoplesIQs)
```

>  What is the probability of observing a *mean* IQ score as high as we observed given the *null hypothesis* that there was no effect of the treatment?


```{r}
plot(density(rnorm(10000,100,15)), ylim=c(0,0.05))
lines(density(peoplesIQs), col=2)
abline(v=mean(peoplesIQs), col=2)
abline(v=100)

```

-> We want to compare the mean of the group we have with means of random groups from the population. 
central limit theorem tells us that the means from the population samples are distributed normally, with mean = mean of the sample and variance = var(sample)/sqrt(N). 


```{r}

```



So there is a 3.8 in one million chance that we could have observed a mean IQ as large as we did if we had sampled 20 people from a population of individuals who were not affected by the treatment. Again, we compare the observed probability to our $\alpha$ level, and make a decision to reject (or not reject) the null hypothesis that the treatment had no effect. In this case if our $\alpha$ level was 0.05, we would reject the null hypothesis and conclude the treatment indeed had an effect.

Note that the logic of hypothesis testing fundamentally depends on the assumption that our sample is a true random sample from the population. If the mean IQ in the population is 100 but we take a sample of 20 from first year university students, this would represent a biased sample. The IQ scores of first year university students are likely not representative of the population of humans as a whole.


Typically we do not know the mean and standard deviation of the population, we have to estimate them from our sample. The best estimate of the population mean is the sample mean. The best estimate of the population standard deviation is *the standard error of the sampling distribution of the mean*. For very large samples (e.g. N>100) this is fairly accurate. For small samples it is not. Another theoretical sampling distribution exists that is appropriate for these situations: the t-distribution.


## The t-distribution

The t-distribution is similar to the normal distribution, however there is a different shape for each sample size N.

Let's do the same example as above: We sample 20 subjects at random from the population. 

Let's assume we don't know the population standard deviation. 

We assume the population is normally distributed. Let's compute the probability of observing a mean IQ of 115 (15 points higher than the supposed population mean) or higher given a sample of size N=20 and sd=30.

\begin{equation}
	t = \frac{\bar{X}_{1}-\mu}{sd/\sqrt{N}}
\end{equation}

```{r}
(tobs <- (115-100)/(sd(peoplesIQs)/sqrt(20)))
(p <- 1-pt(tobs, 19))


```

In this case we see that we get pretty simmilar results both by t distribution and normal distribution.


-------------------------------------------------------------------------------------------------------------------------------------




Before we show how to construct a confidence interval for the difference between the two groups, we will show how to construct a confidence interval for the population mean of control female mice. Then we will return to the group difference after we've learned how to build confidence intervals in the simple case.


#### Confidence Interval for Population Mean

Lets see how good is central limit theorem. Assume we are interested in determining the mean of the bodyweight of the entire untreated mice female population. (untreated are the ones on the "chow" diet = normal diet.)


```{r}
library(data.table)
mouseData <-  fread("https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/mice_pheno.csv")
PopulationBodyweight <- mouseData[Sex=="F" & Diet=="chow", Bodyweight]

```


The population average $\mu_X$ is our parameter of interest here:

```{r}
mu_chow <- mean(PopulationBodyweight)
mu_chow
```


We are interested in estimating this parameter. In practice, we do not get to see the entire population so, as we did for p-values, we demonstrate how we can use samples to do this. Let's start with a sample of size 30:

```{r}
N <- 30
onesample <- sample(PopulationBodyweight,N)
print(mean(onesample))

```


We know this is a random variable, so the sample average will not be a perfect estimate. In fact, because in this illustrative example we know the value of the parameter, we can see that they are not exactly the same. A confidence interval is a statistical way of reporting our finding, the sample average, in a way that explicitly summarizes the variability of our random variable.

With a sample size of 30, we will use the CLT. The CLT tells us that $\bar{X}$ or `mean(chow)` follows a normal distribution with mean $\mu_X$ or `mean(chowPopulation)` and standard deviation approximately  $s_X/\sqrt{N}$ or:

So we know its distribution: 

```{r}
meannew <- mean(onesample)
sdnew <- sd(chow)/sqrt(N)
plot(density(rnorm(10000,meannew,sdnew)))

```

The question now is: what values will this mean have 95% of thew times? This is called a 95% confidence interval.

This is very easy to do in R:
since we want 95% confidence interval, we want to leave out bottom 2.5% = 0.025 / 1 and top 2.5 % = 0.975 / 1

```{r}
bottomboundary <- qnorm(0.025,meannew, sdnew)
topboundary <- qnorm(0.975,meannew, sdnew)
c(bottomboundary, topboundary)
```

And there you have it. 95% of the samples of size 30 will have the mean between 22.56 and 25.04. 

Confdence interval (we can use percentages other than 95%) is a random interval with a 95% probability of falling on the parameter we are estimating. Keep in mind that saying 95% of random intervals will fall on the true value (our definition above) is *not the same* as saying there is a 95% chance that the true value falls in our interval. 



Since the central limit theorem does not work the best with small sample sizes, use the t distribution instead of the normal distribution for small sample sizes. (~ 30).



-------------------------------------------------------------------------------------------------------------------------------------



## Confidence intervals for the mean exercise

Calculate 95% confidence interval for the mean of the population IQs from sample saved in peoplesIQs variable.

```{r}
set.seed(1)
peoplesIQs <- sample(100:130, 40, replace = T)

N <- 40
# approximate population mean, same as the sample mean :
newmean <- mean(peoplesIQs)

# approximate sd, sd=sd_sample/sqrt(N)
newsd <- sd(peoplesIQs)/sqrt(N)

# write the interval here
c(qnorm(0.025,newmean, newsd), qnorm(0.975,newmean, newsd))

```

### In T distribution it would look like this:

The degrees of freedom for the t-statistic are N-1. Note also we divide our $\alpha$ by two in order to get both tails of the t-distribution.

```{r}

tcrit <- qt(0.975, df=N-1)
(cim <- c(newmean-(tcrit*newsd), newmean+(tcrit*newsd)))

```


so as you can see it is very similar for large N..


-------------------------------------------------------------------------------------------------------------------------------------

## t-test for difference between groups

Assume we have two random samples, and we want to test the hypotheses that these samples have been drawn from the same population (null hypothesis) or different populations (alternate hypothesis).

There are some important assumptions of the t test. They have to be valid if you plan to trust the results of this test:

- the data you will be testing should follow a continuous or ordinal scale, such as the scores for an IQ test.

- the data is collected from a representative, randomly selected portion of the total population.

- data is normaly distributed when plotted

- resonably large sample size (resembles a bell shaped curve when plotted)

- The final assumption is homogeneity of variance. Homogeneous, or equal, variance exists when the standard deviations of samples are approximately equal.


In R it is dead easy to run a t-test, using the `t.test()` function:

```{r}
g1 <- c(5,4,4,6,5)
g2 <- c(6,7,5,8,7)
t.test(g1, g2, alternative="two.sided", paired=FALSE, var.equal=TRUE)
```

Note that we have to tell `t.test()` whether we want a one-tailed or two-tailed test, whether the groups are correlated (paired) or independent, and whether or not we want to assume that group variances are equal or not. The homogeneity of variances is an underlying assumption of the t-test. If it is violated you can simply tell `t.test()` that `var.equal=FALSE` and it will run a corrected version of the test. You can test the homogeneity of variances assumption using `bartlett.test()`:

```{r}
bartlett.test(c(g1,g2), c(rep(1,5), rep(2,5)))

```

In this case p=0.5625 so we do not reject the null hypothesis that the variances are equal---in other words homogeneity of variance has not been violated.

### Paired t-test

When the two groups are correlated, as in when the same subject contributes a score in each group, we simply pass `t.test()` the argument `paired=TRUE`:

```{r}
t.test(g1, g2, alternative="two.sided", paired=TRUE, var.equal=TRUE)

```

## Testing the normality assumption

In R we can test the normality assumption using `shapiro.test()`:

```{r}
g3 <- c(5,6,8,7,6,3,4,5,6,7,6,5,5,6,7,9,3,2)
shapiro.test(g3)
```

The p-value is 0.6856 which is greater than our $\alpha$ level of 0.05, so we fail to reject the null hypothesis that the sample was drawn from a normal population.

Another visual method is to generate a normal quantile-quantile plot:

```{r}
qqnorm(g3)
qqline(g3, lty=2)
```

If the sample is normally distributed, the data points should all fall along the dashed line. If the data are not normally distributed, then a non-parametric test is more appropriate than the t-test---e.g the `wilcox.test()`.

-------------------------------------------------------------------------------------------------------------------------------------
#### Exercise:

Here are some random 100 temperatures in fahrenheits:

```{r}
set.seed(1)
tempF <- rnorm(100,65, 3.5)
```

Plot the density of the heights:
```{r}
plot(density(tempF))
```

Convert the temperatures to celsius degrees and plot it. 
(The temperature T in degrees Celsius (°C) is equal to the temperature T in degrees Fahrenheit (°F) minus 32, times 5/9:)

```{r}
tempC <- (tempF - 32)*5/9
plot(density(tempC))
```

Now plot them both on the same graph. Do they look the same? (add xlim=c(0,100) inside the plot function). (Add the second ploit with lines function )

```{r}
plot(density(tempC), xlim=c(0,100))
lines(density(tempF), col=2)
```

So, are they the same or no? Should it matter if we are measuring in celsius or fahrenheits? Here are some other temperatures. They are the same as first ones, but I added 10°C to each temperature. Will t test give you same value in fahrenheits and in celsius?

Test the assumptions for t test before conducting it!

```{r}
set.seed(12)
temp2C <- tempC+rnorm(50,0.8,2)
temp2F <- temp2C*9/5+32

# test the assumptions


# do the test for fahrenheit

# do the test for celsius

```

Hmmm... Hm hm hm... close... We just got some new samples, can you try it on everything:


```{r}
set.seed(3)
newsampletoadd <- rnorm(50, mean(tempC), sd(tempC) )
tempCnew <- c(tempC, newsampletoadd)    



```

#### Any problem?

Oh, I just remembered the data points are paired, we measured the temperatures before and after we did something...Does that change anything?

```{r}

```





-------------------------------------------------------------------------------------------------------------------------------------

This workshop is based on the materials (chapter I of the book) provided by Rafael Irizarry and Michael Love and modified for the IGNITE workshop students by Maja Kuzman. It was originally used for the HarvardX series PH525x http://genomicsclass.github.io/book and is licenced under the following MIT license.


The MIT License (MIT)
Copyright (c) 2013 Rafael Irizarry and Michael Love
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: 
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. 
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. 